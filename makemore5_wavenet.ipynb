{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff132186-1b7b-40a6-b854-0a1382195989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ae07f-e747-4c44-a056-08eafff5787b",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2be0117-7637-4c69-90b9-e6e5bcc356e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dff0f4a-8140-4b52-ba6b-c6270203250a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2382afe-b838-442a-ab52-5e04ae16fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "def build_dataset(words):\n",
    "    block_size = 3\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for w in words:\n",
    "    \n",
    "        context = [0] * block_size\n",
    "        for ch in w+\".\":\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "    \n",
    "            context = context[1:]+[ix]\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)   \n",
    "\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054654e2-ba55-44b9-a84c-b8a22d0dafb3",
   "metadata": {},
   "source": [
    "### Defining the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55609dde-6e52-4111-a445-e895483f830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from makemore.linear import Linear\n",
    "from makemore.tanh import Tanh\n",
    "from makemore.batchNorm1d import BatchNorm1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2755d51b-aab8-4a97-b6df-1b4488b8de86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47024\n"
     ]
    }
   ],
   "source": [
    "n_embed = 10\n",
    "n_hidden = 100\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((vocab_size, n_embed), generator=g)\n",
    "\n",
    "\n",
    "layers = [\n",
    "  Linear(n_embed * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "  # last layer: make less confident\n",
    "  layers[-1].gamma *= 0.1\n",
    "  #layers[-1].weight *= 0.1\n",
    "  # all other layers: apply gain\n",
    "  for layer in layers[:-1]:\n",
    "    if isinstance(layer, Linear):\n",
    "      layer.weight *= 1.0 #5/3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd61a86-2392-47d3-a331-6bac90e12ab6",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29862502-688b-4cc7-93d1-96f5c5c63e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 2.1189\n",
      "  10000/ 200000: 2.4210\n",
      "  20000/ 200000: 1.8325\n",
      "  30000/ 200000: 1.9717\n",
      "  40000/ 200000: 1.9369\n",
      "  50000/ 200000: 2.8048\n",
      "  60000/ 200000: 1.9833\n",
      "  70000/ 200000: 2.0637\n",
      "  80000/ 200000: 2.3088\n",
      "  90000/ 200000: 2.2504\n",
      " 100000/ 200000: 2.0926\n",
      " 110000/ 200000: 2.0370\n",
      " 120000/ 200000: 1.8982\n",
      " 130000/ 200000: 1.5689\n",
      " 140000/ 200000: 2.0867\n",
      " 150000/ 200000: 1.9026\n",
      " 160000/ 200000: 1.6841\n",
      " 170000/ 200000: 1.5990\n",
      " 180000/ 200000: 1.8545\n",
      " 190000/ 200000: 2.1288\n",
      "CPU times: user 55min 2s, sys: 13.5 s, total: 55min 15s\n",
      "Wall time: 9min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "\n",
    "    #minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    emb = C[Xb]\n",
    "    x = emb.view(-1, block_size * n_embed)\n",
    "\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    \n",
    "    loss = F.cross_entropy(x, Yb)\n",
    "\n",
    "     # backward pass\n",
    "    for layer in layers:\n",
    "      layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
    "      \n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    lr = 0.1 if i<100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    with torch.no_grad(): #checks how large are the update compared to values\n",
    "        ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "\n",
    "    # # if i >= 1000:\n",
    "    # #   break # AFTER_DEBUG: would take out obviously to run full optimization\n",
    "    # if i % 10000 == 0: # print every once in a while\n",
    "    #   print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    # lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d72d2-358f-46c5-ba50-239dd39b8289",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f088497-14d1-49ff-9215-94976c378fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layers:\n",
    "    layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c65969a-e3c8-4157-b35a-7944b0ee59c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1.96268630027771\n",
      "dev 2.09045672416687\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "       \"train\" : (Xtr, Ytr),\n",
    "        \"test\" : (Xte, Yte),\n",
    "        \"dev\": (Xdev, Ydev)\n",
    "    }[split] \n",
    "    emb = C[x]\n",
    "    x = emb.view(emb.shape[0], -1)\n",
    "\n",
    "    for layer in layers:\n",
    "        x = layer(x)    \n",
    "    \n",
    "    loss = F.cross_entropy(x, y)\n",
    "    print(split, loss.item()) \n",
    "\n",
    "split_loss(\"train\")\n",
    "split_loss(\"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca18dd61-f77a-4cdf-9c07-c7527fcad845",
   "metadata": {},
   "source": [
    "### Sample from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2d79116-e3ce-4250-8e0a-bd26f9c5247d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "av\n",
      "avr\n",
      "avri\n",
      "avrin\n",
      "avring\n",
      "avrings\n",
      "avringst\n",
      "avringste\n",
      "avringstel\n",
      "avringstell\n",
      "avringstelle\n",
      "avringstellen\n",
      "a\n",
      "ar\n",
      "ari\n",
      "aria\n",
      "ariau\n",
      "ariaun\n",
      "b\n",
      "be\n",
      "ben\n",
      "bent\n",
      "benty\n",
      "bentyn\n",
      "a\n",
      "an\n",
      "ani\n",
      "aniy\n",
      "aniya\n",
      "aniyah\n",
      "g\n",
      "ge\n",
      "gem\n",
      "gemi\n",
      "gemia\n",
      "gemiah\n",
      "s\n",
      "st\n",
      "sto\n",
      "ston\n",
      "stone\n",
      "s\n",
      "so\n",
      "sor\n",
      "sorr\n",
      "sorre\n",
      "sorres\n",
      "sorresk\n",
      "sorreska\n",
      "m\n",
      "ma\n",
      "mar\n",
      "mari\n",
      "k\n",
      "ka\n",
      "kam\n",
      "kama\n",
      "kamar\n",
      "kamark\n",
      "kamarke\n",
      "kamarkel\n",
      "d\n",
      "da\n",
      "dax\n",
      "j\n",
      "ja\n",
      "jak\n",
      "jaky\n",
      "jakyr\n",
      "jakyri\n",
      "jakyria\n",
      "jakyrian\n",
      "jakyriann\n",
      "jakyrianne\n",
      "jakyrianner\n",
      "n\n",
      "ny\n",
      "nyl\n",
      "nyla\n",
      "nylan\n",
      "nylani\n",
      "k\n",
      "ka\n",
      "kai\n",
      "kair\n",
      "kaire\n",
      "kairel\n",
      "m\n",
      "ma\n",
      "mav\n",
      "mave\n",
      "mavee\n",
      "r\n",
      "ra\n",
      "rae\n",
      "raey\n",
      "raeyi\n",
      "raeyio\n",
      "t\n",
      "th\n",
      "tha\n",
      "thai\n",
      "thail\n",
      "thaila\n",
      "thailan\n",
      "thailana\n",
      "a\n",
      "al\n",
      "ale\n",
      "alem\n",
      "alemo\n",
      "alemon\n",
      "alemone\n",
      "alemoneo\n",
      "d\n",
      "de\n",
      "des\n",
      "dest\n",
      "desto\n",
      "deston\n",
      "destoni\n",
      "destonis\n",
      "h\n",
      "ha\n",
      "han\n",
      "hana\n",
      "hanat\n",
      "hanata\n",
      "hanatal\n",
      "hanatale\n",
      "hanatalek\n",
      "hanataleko\n",
      "hanatalekob\n",
      "hanatalekobe\n",
      "hanatalekober\n",
      "m\n",
      "me\n",
      "mel\n",
      "melo\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    context = [0]*block_size\n",
    "    out = []\n",
    "\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "\n",
    "        x = emb.view(emb.shape[0], -1)\n",
    "        for layer in layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        logits = x\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "\n",
    "        if ix==0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(itos[i] for i in out))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5421f-c9eb-4c46-8632-c2fdb9efd1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
